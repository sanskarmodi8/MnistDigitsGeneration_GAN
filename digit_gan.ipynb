{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 16:06:36.592858: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-06 16:06:36.628861: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-06 16:06:36.802841: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-06 16:06:36.802887: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-06 16:06:36.803854: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-06 16:06:36.894122: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-06 16:06:36.895438: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-06 16:06:37.747149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available and TensorFlow is using it.\")\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPU available; TensorFlow is running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32') # reshape to 28x28x1\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-06 16:10:44.871808: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "#take the dataset, shuffle it, and batch it to 256\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256 \n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) #now train dataset everytime we call it, it will give us another batch of 256 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 12544)             1254400   \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 12544)             50176     \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 12544)             0         \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 7, 7, 256)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_12 (Conv2  (None, 7, 7, 128)         819200    \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 7, 7, 128)         512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_13 (Conv2  (None, 14, 14, 64)        204800    \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 14, 14, 64)        256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_transpose_14 (Conv2  (None, 28, 28, 1)         1600      \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2330944 (8.89 MB)\n",
      "Trainable params: 2305472 (8.79 MB)\n",
      "Non-trainable params: 25472 (99.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # take a random 100 dimensional noise vector and turns it into a 7x7x256 dimensional vector\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization()) \n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    \n",
    "    # upsample to 7x7x128\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    # upsample to 14x14x64\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    # upsample to 28x28x1\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ffa346a63e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predicting some random generated images and visualizing them \n",
    "noise = tf.random.normal([1, 100]) # tf.random.normal([1, 100]) creates a 1x100 dimensional vector of random numbers\n",
    "generated_image = generator(noise, training=False) # generate an image from the noise vector\n",
    "print(generated_image.shape) # print the shape of the generated image\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray') # plot the image # generated_image[0, :, :, 0] is the first image in the batch, the :, :, 0 is to remove the color dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 14, 14, 64)        1664      \n",
      "                                                                 \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 128)         204928    \n",
      "                                                                 \n",
      " leaky_re_lu_16 (LeakyReLU)  (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 6273      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 212865 (831.50 KB)\n",
      "Trainable params: 212865 (831.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1])) # 28x28x1 -> 14x14x64\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same')) # 14x14x64 -> 7x7x128\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Flatten()) # 7x7x128 -> 6272\n",
    "    model.add(layers.Dense(1)) # 6272 -> 1\n",
    "    \n",
    "    return model\n",
    "\n",
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.00112521]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# predicting if the generated image is real or fake\n",
    "decision = discriminator(generated_image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True) # from_logits=True will make the output of the discriminator not be normalized between 0 and 1 because we are going to use the sigmoid activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output) # tf.ones_like(real_output) creates a vector of 1s the same size as real_output\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) # tf.zeros_like(fake_output) creates a vector of 0s the same size as fake_output\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers\n",
    "g_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "d_opt = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=g_opt,\n",
    "                                 discriminator_optimizer=d_opt,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500 \n",
    "noise_dim = 100 \n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# we will use this to visualize the progress of the generator as a gif\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # tf.function is a decorator that tells TensorFlow to compile this function\n",
    "def train_step(images): # train_step is a function that takes in a batch of images and trains the generator and discriminator\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "        generated_images = generator(noise, training=True) # training=True tells the model that we are training it so that it will use dropout\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    g_grads = g_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    d_grads = d_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    g_opt.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "    d_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time() # start the timer\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "        display.clear_output(wait=True) # clear the output of the cell #wait=True will wait to clear the output until a new output is available to replace it with\n",
    "        generate_and_save_images(generator, epoch + 1, seed) # generate and save images after each epoch\n",
    "        if (epoch + 1) % 15 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix) # save the model every 15 epochs\n",
    "        print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start)) # print the time it took to train the epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator, epochs, seed) # generate and save images after the last epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input): # generate and save images\n",
    "    predictions = model(test_input, training=False) # generate images from the test_input\n",
    "    fig = plt.figure(figsize=(4,4)) # create a figure\n",
    "    for i in range(predictions.shape[0]): # loop through the images\n",
    "        plt.subplot(4, 4, i+1) # create a subplot\n",
    "        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray') # plot the image\n",
    "        plt.axis('off') # turn off the axis\n",
    "    plt.savefig('images/image_at_epoch_{:04d}.png'.format(epoch)) # save the figure\n",
    "    plt.show() # show the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "# train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore the latest checkpoint\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a GIF to visualize the generator predictions over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan_mnist_digits.gif'\n",
    "with imageio.get_writer(anim_file, mode='I') as writer: # get_writer is a function that creates a writer object that will convert the images to a gif, mode='I' tells the writer that the images are from a sequence\n",
    "    filenames = glob.glob('images/image*.png') # glob.glob('images/image*.png') will create a list of all the filenames that match the pattern 'images/image*.png'\n",
    "    filenames = sorted(filenames)\n",
    "    last = -1 #initializes the last variable, which will keep track of the frame number of the last image processed.\n",
    "    for i,filename in enumerate(filenames):\n",
    "        frame = 2*(i**0.5) #creates the frame number for this image using a simple formula which will increase the frame number by 2 every time the square root of i increases by 1\n",
    "        if round(frame) > round(last): #checks if the current frame is greater than the last frame processed. It ensures that you don't append the same frame multiple times\n",
    "            last = frame #updates the last variable\n",
    "        else:\n",
    "            continue #skips the current image if it is the same frame as the last image\n",
    "        image = imageio.imread(filename) #reads the image\n",
    "        writer.append_data(image) #appends the image to the writer object\n",
    "    \n",
    "    \n",
    "import tensorflow_docs.vis.embed as embed\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of gen and disc losses to see of they converge\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(g_losses, label='Generator Loss')\n",
    "plt.plot(d_losses, label='Discriminator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save('generator.h5')\n",
    "discriminator.save('discriminator.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
